{% include lib/mathjax.html %}
# ขอบเขตของ Growth Function

หากเราย้อนกลับมาดู generalization bound ของ error ของ hypothesis ที่ได้จากการเรียนรู้แบบ PAC
ในกรณีที่ hypothesis space $H$ มีขนาดจำกัด เราสรุปได้ว่าหากมี hypothesis $c\in H$ ที่ $R(c)=0$ อยู่จริง
เราสามารถทำการเรียนรู้โดยการหา hypothesis $h$ ที่ consistent กับตัวอย่างข้อมูลทั้งหมด
ซึ่งจะทำให้ได้ผลลัพธ์เป็น $h\in H$ ที่รับประกันได้ว่า ด้วยความน่าจะเป็นไม่น้อยกว่า $1-\delta$

$$
R(h)\leq \frac{1}{m}(\ln|H| + \ln\frac{1}{\delta})
$$

ในขณะที่กรณีของ Agnostic-PAC สำหรับ hypothesis $h\in H$ ใด ๆ ที่มี empirical error เป็น $\hat{R}(h)$
เราสามารถสรุปขอบเขตของ error จริงของ $h$ ได้ว่า ด้วยความน่าจะเป็นไม่น้อยกว่า $1-\delta$

$$
R(h)\leq \hat{R}(h) + \sqrt{\frac{1}{2m}(\ln |H| + \frac{2}{\delta})}
$$

เมื่อ $H$ ของเรามีขนาดเป็นอนันต์ เราจำเป็นจะต้องหาเครื่องมือสำหรับวัดความซับซ้อนของ $H$ มาใช้แทน
$|H|$ ซึ่งทางเลือกหนึ่งที่น่าสนใจคือการใช้ growth function $\Pi_H(m)$ เนื่องจาก $\Pi_H(m)$
นั้นสื่อถึงของเขตของขนาดของ $H$ เมื่อพิจารณาบนตัวอย่างข้อมูลจำนวนจำกัด
หากลองคิดเล่น ๆ ว่าเราจะนำค่า $\Pi_H(m)$ มาแทนที่ $|H|$ ตรง ๆ ใน error bar
$\frac{1}{m}(\ln|H| + \ln\frac{1}{\delta})$
และ $\sqrt{\frac{1}{2m}(\ln |H| + \frac{2}{\delta})}$
จะเห็นว่า ถ้า $H$ มีความซับซ้อนมากเช่น
$\Pi_H(m)=2^m$ เสมอ เราจะได้ว่า error bar ของการเรียนรู้ทั้งสองแบบจะไม่ลู่เข้าสู่ศูนย์เมื่อจำนวนตัวอย่างข้อมูล $m$
มีค่ามากขึ้น แต่ถ้า hypothesis space $H$ ของเรามี $\Pi_H(m)$ เป็น polynomial บน $m$
เราก็จะยังสามารถลด error bar ด้วยการเพิ่มจำนวนข้อมูล $m$ ได้

ในหัวข้อนี้เราจะแสดงให้เห็นว่า ถ้า hypothesis space $H$ มี VC-dimension เป็นจำนวนจำกัด (ไม่ใช่ $\infty$)
ค่าของ $\Pi_H(m)$ จะมีอัตราการโตเป็น polynomial บน $m$ 
