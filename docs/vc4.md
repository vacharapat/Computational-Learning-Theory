{% include lib/mathjax.html %}
# ขอบเขตของ Growth Function

หากเราย้อนกลับมาดู generalization bound ของ error ของ hypothesis ที่ได้จากการเรียนรู้แบบ PAC
ในกรณีที่ hypothesis space $H$ มีขนาดจำกัด เราสรุปได้ว่าหากมี hypothesis $c\in H$ ที่ $R(c)=0$ อยู่จริง
เราสามารถทำการเรียนรู้โดยการหา hypothesis $h$ ที่ consistent กับตัวอย่างข้อมูลทั้งหมด
ซึ่งจะทำให้ได้ผลลัพธ์เป็น $h\in H$ ที่รับประกันได้ว่า ด้วยความน่าจะเป็นไม่น้อยกว่า $1-\delta$

$$
R(h)\leq \frac{1}{m}(\ln|H| + \ln\frac{1}{\delta})
$$

ในขณะที่กรณีของ Agnostic-PAC สำหรับ hypothesis $h\in H$ ใด ๆ ที่มี empirical error เป็น $\hat{R}(h)$
เราสามารถสรุปขอบเขตของ error จริงของ $h$ ได้ว่า ด้วยความน่าจะเป็นไม่น้อยกว่า $1-\delta$

$$
R(h)\leq \hat{R}(h) + \sqrt{\frac{1}{2m}(\ln |H| + \frac{2}{\delta})}
$$

เมื่อ $H$ ของเรามีขนาดเป็นอนันต์ เราจำเป็นจะต้องหาเครื่องมือสำหรับวัดความซับซ้อนของ $H$ มาใช้แทน
$|H|$ ซึ่งทางเลือกหนึ่งที่น่าสนใจคือการใช้ growth function $\Pi_H(m)$ เนื่องจาก $\Pi_H(m)$
นั้นสื่อถึงของเขตของขนาดของ $H$ เมื่อพิจารณาบนตัวอย่างข้อมูลจำนวนจำกัด
หากลองคิดเล่น ๆ ว่าเราจะนำค่า $\Pi_H(m)$ มาแทนที่ $|H|$ ตรง ๆ ใน error bar
$\frac{1}{m}(\ln|H| + \ln\frac{1}{\delta})$
และ $\sqrt{\frac{1}{2m}(\ln |H| + \frac{2}{\delta})}$
จะเห็นว่า ถ้า $H$ มีความซับซ้อนมากเช่นเมื่อ VC-dimension ของ $H$ เป็น $\infty$ หรือ
$\Pi_H(m)=2^m$ เสมอ เราจะได้ว่า error bar ของการเรียนรู้ทั้งสองแบบจะไม่ลู่เข้าสู่ศูนย์เมื่อจำนวนตัวอย่างข้อมูล $m$
มีค่ามากขึ้น แต่ถ้า hypothesis space $H$ ของเรามี $\Pi_H(m)$ เป็น polynomial บน $m$
เราก็จะยังสามารถลด error bar ด้วยการเพิ่มจำนวนข้อมูล $m$ ได้

ในหัวข้อนี้เราจะแสดงให้เห็นว่า ถ้า hypothesis space $H$ มี VC-dimension เป็นจำนวนจำกัด (ไม่ใช่ $\infty$)
ค่าของ $\Pi_H(m)$ จะมีอัตราการโตเป็น polynomial บน $m$

## ตัวอย่างการนับ $\Pi_H(m)$ สำหรับ $H$ ที่มี VC-dimension เป็นจำนวนจำกัด
ก่อนที่จะพิสูจน์ให้เห็นว่าสำหรับ $H$ ใด ๆ ที่มี VC-dimension เป็นจำนวนจำกัดนั้น $\Pi_H(m)$ เป็น polynomial บน $m$
เราจะมาลองดูตัวอย่างในกรณีง่าย ๆ กันก่อนเพื่อที่จะได้เห็นว่า เงื่อนไขจาก VC-dimension ที่เป็นจำนวนจำกัดนั้น
ทำให้จำนวน dichotomy ที่สามารถสร้างได้จาก $H$ นั้นน้อยกว่า $2^m$ อยู่มาก

สมมติให้ $H$ เป็น hypothesis space ที่มี VC-dimension เท่ากับ 1 นั่นคือ สำหรับตัวอย่างข้อมูล 2 ตัวใด ๆ
$H$ จะต้องไม่สามารถสร้าง dichotomy ของตัวอย่างข้อมูลทั้งสองตัวได้ครบทุกรูปแบบ
คราวนี้เราลองมานับจำนวน dichotomy ของตัวอย่างข้อมูลจำนวนสามตัวที่สร้างได้จาก $H$
เราจะลองนับโดยการพิจารณาการให้ label ของตัวอย่างข้อมูล $x_1,x_2,x_3$ ตามลำดับ $(0,0,0), (0,0,1),\dots,(1,1,1)$
โดยที่ หากการให้ label แบบใดทำให้มีตัวอย่างข้อมูลคู่หนึ่งที่มี dichotomy ครบทุกแบบเกิดขึ้น ซึ่งแสดงว่า $H$ shatter ข้อมูลคู่นั้นได้ เราจะตัดการให้ label นั้นออกไป

เราจะแสดงผลการนับตามที่กล่าวมาด้วยตารางต่อไปนี้

|$x_1,x_2,x_3$|เป็น dichotomy ของ $H$ ได้หรือไม่|
|:-----------:|:---------------------------:|
| $0,0,0$ | ได้ |
| $0,0,1$ | ได้ |
| $0,1,0$ | ได้ |
| $0,1,1$ | ไม่ได้ เนื่องจากทำให้ $H$ shatter $$\{x_2,x_3\}$$|
| $1,0,0$ | ได้ |
| $1,0,1$ | ไม่ได้ เนื่องจากทำให้ $H$ shatter $$\{x_1,x_3\}$$|
| $1,1,0$ | ไม่ได้ เนื่องจากทำให้ $H$ shatter $$\{x_1,x_2\}$$|
| $1,1,1$ | ไม่ได้ เนื่องจากทำให้ $H$ shatter $$\{x_1,x_2\}$$|
