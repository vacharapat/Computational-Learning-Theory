{% include lib/mathjax.html %}
# Dichotomy

ในการวิเคราะห์ sample complexity หรือ generalization bound ที่ผ่านมาทั้งหมดนั้น
ขนาดของ hypothesis space $H$ ที่เราเลือกใช้มีบทบาทสำคัญอย่างมาก ตัวอย่างเช่นในการเรียนรู้แบบ
agnostic-PAC เราสามารถสรุปได้ว่า สำหรับ hypothesis $h\in H$ ใด ๆ เมื่อเราทำการเรียนรู้จากตัวอย่างข้อมูลจำนวน $m$ ตัว
เราจะได้ว่าด้วยความน่าจะเป็นไม่น้อยกว่า $1-\delta$

$$
R(h)\leq \hat{R}(h)+\sqrt{\frac{1}{2m}(\ln|H|+\ln\frac{2}{\delta})}
$$

เราอาจเรียกพจน์ $\sqrt{\frac{1}{2m}(\ln|H|+\ln\frac{2}{\delta})}$ ว่า _error bar_
ซึ่งทำหน้าที่บอกขอบเขตของความแตกต่างระหว่าง $R(h)$ ซึ่งเป็นค่า error จริงของ $h$ ที่เราไม่สามารถเข้าถึงได้
กับ $\hat{R}(h)$ ที่เราสามารถวัดได้จากตัวอย่างข้อมูล สังเกตว่า error bar นี้ขึ้นกับ $m$ และ $|H|$
ดังนั้นหากเราต้องการให้ error bar นี้มีค่าน้อย
เราสามารถทำได้โดยการเพิ่มจำนวนข้อมูลในการเรียนรู้ (เพิ่มค่าของ $m$) หรือเลือก hypothesis space
ที่มีขนาดเล็ก (ทำให้ $|H|$ มีค่าน้อย) ซึ่งก็จะมี trade-off ระหว่าง estimation error และ approximation error
ดังที่เราได้ศึกษากันไปแล้ว
อย่างไรก็ดี hypothesis space ที่ใช้จริงในทางปฏิบัติจำนวนมากมีขนาดเป็นอนันต์ ซึ่ง error bar นี้ไม่สามารถใช้กับ
hypothesis space เหล่านี้ได้ จึงเป็นปัญหาตามมาว่า สำหรับ hypothesis space ที่มีขนาดเป็นอนันต์เหล่านี้
เราจะสามารถทำการเรียนรู้ตามกรอบของ PAC ได้อยู่หรือไม่

ในการวิเคราะห์ [ปัญหาการเรียนรู้รูปสี่เหลี่ยมขนานแกน](https://vacharapat.github.io/Computational-Learning-Theory/docs/pac2)
ทำให้เราเห็นว่าอย่างน้อยที่สุด ถึงแม้ว่าขนาดของ hypothesis space จะเป็นอนันต์ เราก็จะยังสามารถทำการเรียนรู้ได้ในบางกรณี
เป้าหมายหลักของบทนี้คือ เราจะพยายามขยายผลของการวิเคราะห์ปัญหาการเรียนรู้ของเราให้รองรับการวิเคราะห์บน hypothesis space
เหล่านี้ได้

## ปัญหาของ Union bound
ในกรณีที่ hypothesis space $H$ มีขนาดจำกัดนั้น การวิเคราะห์ของเราหาขอบเขตความน่าจะเป็นที่คำตอบจากอัลกอริทึมใด ๆ จะเป็น hypothesis $h\in H$ ที่มี $|R(h)-\hat{R}(h)|>\epsilon$ โดยการจำกัดขอบเขตของความน่าจะเป็นที่จะมี hypothesis สักตัวเป็นไปตามเงื่อนไขนั้น
กล่าวคือ ถ้าเราให้ $B_i$ แทนเหตุการณ์ที่ $|R(h_i)-\hat{R}(h_i)|>\epsilon$
ในการวิเคราะห์ของเรา เราพยายามจำกัดขอบเขตความน่าจะเป็นของเหตุการณ์

$$
B_1 \text{ หรือ }
B_2 \text{ หรือ }
\dots\text{ หรือ }
B_{|H|}
$$

โดยเราใช้ union bound เป็นขอบเขตว่าความน่าจะเป็นของเหตุการณ์ดังกล่าวนี้จะมีค่า

$$
\Pr[\bigcup_{i=1}^{|H|}B_i]\leq\sum_{i=1}^{|H|}\Pr[B_i]
$$

เมื่อเราสามารถหาขอบเขตของความน่าจะเป็นของเหตุการณ์ย่อย $B_i$ แต่ละตัวได้เท่ากัน จึงเป็นที่มาที่ทำให้มีค่าของ
$|H|$ ปรากฏใน generalization bound

อย่างไรก็คือ หาก
