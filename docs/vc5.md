{% include lib/mathjax.html %}
# VC Generalization Bound

จากหัวข้อที่แล้ว เราเห็นแล้วว่าอัตราการโตของ $\Pi_H(m)$ นั้นเป็น polynomial บน $m$
ซึ่งทำให้เรามีความหวังที่จะนำค่า $\Pi_H(m)$ มาใช้เป็นเครื่องแสดงความซับซ้อนของ hypothesis space $H$
แทนขนาดของ $H$ ได้ อย่างไรก็ดีเราไม่สามารถทำการวิเคราะห์ generalization bound ของ hypothesis
ที่ได้จากการเรียนรู้โดยใช้ union bound เช่นเดิมได้เนื่องจากขนาดของ $H$ อาจมีค่าเป็นอนันต์  ในหัวข้อนี้เราจะมาดูวิธีการวิเคราะห์ generalization bound ให้อยู่ในรูปของ $\Pi_H(m)$ แทน

## Consistent hypothesis
เราจะเริ่มพิจารณาในกรณีที่เราทราบว่ามี concept เป้าหมาย $c\in H$ อยู่จริง และเราทำการเรียนรู้โดยการหา
hypothesis $h\in H$ ที่ consistent กับตัวอย่างข้อมูลทั้งหมดที่ได้มา ซึ่งแทนด้วย $$S=\{(x_1,y_1),\dots,(x_m,y_m)\}$$

หากเราต้องการให้ hypothesis $h$ ที่ได้เป็นผลจากการเรียนรู้ของเรามี error $R(h)\leq\epsilon$
เราสามารถทำได้โดยหาขอบเขตของความน่าจะเป็นที่จะมี hypothesis อย่างน้อยตัวหนึ่งใน $H$ ที่มี error มากกว่า $\epsilon$
แต่ยังสามารถเป็นคำตอบจากอัลกอริทึมการเรียนรู้ของเราได้ นั่นคือ hypothesis ดังกล่าวต้อง consistent กับ $S$ ด้วย
เรากล่าวได้อีกอย่างว่าเป้าหมายของเราคือการหาขอบเขตของความน่าจะเป็น

$$
\Pr[\exists h\in H: R(h)>\epsilon \text{ และ } h \text{ consistent กับ } S]
$$

ในกรณีที่ $H$ มีขนาดจำกัด เราสามารถจำกัดขอบเขตของความน่าจะเป็นนี้ได้โดยหาความน่าจะเป็นที่ hypothesis
$h$ ที่ $R(h)>\epsilon$ จะ consistent กับ $S$ และใช้ union bound จำกัดขอบเขตที่จะเกิดเหตุการณ์ดังกล่าวกับ
hypothesis อย่างน้อยหนึ่งตัวได้ แต่เมื่อ $H$ มีขนาดเป็นอนันต์วิธีการนี้ก็ไม่สามารถทำได้
